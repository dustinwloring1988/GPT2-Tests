Name: GPT2-EDU-4-4-2048

class GPTConfig:
    block_size: int = 1024 # max sequence length
    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token
    n_layer: int = 4 # number of layers
    n_head: int = 4 # number of heads
    n_embd: int = 2048 # embedding dimension
    
    
max_lr = 2e-4
min_lr = max_lr * 0.1
warmup_steps = 512
max_steps = 6000 # 190,730 steps is ~1 epoch, if data is 100B tokens and batch size 0.5M tokens


total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens
B = 1 # micro batch size
T = 1024 # sequence length

trainied using 2 4060ti and took 

{
start time: 2024-06-30_00:14:02
end time: 2024-07-01_12:42:31
}
    
    
******* The following was taken from the terminal output upon startiong the training


total desired batch size: 524288
=> calculated gradient accumulation steps: 256
found 1001 shards for split train
found 1 shards for split val
num decayed parameter tensors: 18, with 306,446,336 parameters
num non-decayed parameter tensors: 34, with 110,592 parameters
using fused AdamW: True
validation loss: 11.2555
HellaSwag accuracy: 2570/10042=0.2559

